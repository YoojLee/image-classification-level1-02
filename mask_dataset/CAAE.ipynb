{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad47b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f39ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import pickle\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1a8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.DataLoader import MaskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96cb33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e78ec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b8c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49118f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 42/1766 [00:00<00:04, 416.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask dataset is loading ::::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1766/1766 [00:04<00:00, 424.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MaskDataset(\n",
    "    target         = \"total_label\",\n",
    "    realign        = True,\n",
    "    csv_path       = './train/train.csv',\n",
    "    images_path    = './train/images/',\n",
    "    pre_transforms = transforms.Compose([\n",
    "        #lambda img : transforms.functional.crop(img, 80, 50, 320, 256),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "    ]),\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # transforms.RandomCrop((hp[\"IMAGE_SIZE_H\"],hp[\"IMAGE_SIZE_W\"]), padding=32),\n",
    "        # transforms.RandomRotation(degrees = (-15, 15)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0, 0, 0), (1, 1, 1)),\n",
    "    ]),\n",
    "    sub_mean = False,\n",
    "    debug = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118a6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size  = batch_size,\n",
    "    shuffle     = True,\n",
    "    sampler     = None,\n",
    "    num_workers = 8,\n",
    "    drop_last   = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ab1249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel = 3 # channels of input image\n",
    "n_encode = 64 # channels of conv layer\n",
    "\n",
    "n_z = 60 # dimension of latent vector\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #input: 3*128*128\n",
    "            nn.Conv2d(n_channel,n_encode,5,2,2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(n_encode,2*n_encode,5,2,2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(2*n_encode,4*n_encode,5,2,2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(4*n_encode,8*n_encode,5,2,2),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        self.fc = nn.Linear(8*n_encode*8*8, n_z)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        conv = self.conv(x).view(-1,8*n_encode*8*8)\n",
    "        out = self.fc(conv)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ace50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gen = 64 # channels of convT layers\n",
    "\n",
    "\n",
    "\n",
    "n_age_band = 3 \n",
    "n_mask_band = 3 \n",
    "n_age = 20 # n_age_band * n_age = n_z\n",
    "n_mask = 20 # n_age_band * n_age = n_z\n",
    "n_gender = 30 # 2 * n_gender = n_z\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                n_z + n_age_band*n_age + n_mask_band*n_mask + n_gender,\n",
    "                8*8*n_gen*16\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upconv= nn.Sequential(\n",
    "            nn.ConvTranspose2d(16*n_gen,8*n_gen,4,2,1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(8*n_gen,4*n_gen,4,2,1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(4*n_gen,2*n_gen,4,2,1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(2*n_gen,n_gen,4,2,1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(n_gen,n_channel,3,1,1),\n",
    "            nn.Tanh(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self,z,age,gender,mask):\n",
    "        l = age.repeat(1,n_age)\n",
    "        m = mask.repeat(1,n_mask)\n",
    "        k = gender.view(-1,1).repeat(1,n_gender)\n",
    "        \n",
    "        x = torch.cat([z,l,m,k],dim=1)\n",
    "        fc = self.fc(x).view(-1,16*n_gen,8,8)\n",
    "        out = self.upconv(fc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8064078",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_disc = 16 # channels of conv layer\n",
    "\n",
    "\n",
    "class Dimg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dimg,self).__init__()\n",
    "        self.conv_img = nn.Sequential(\n",
    "            nn.Conv2d(n_channel,n_disc,4,2,1),\n",
    "        )\n",
    "        self.conv_l = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                n_age_band*n_age + n_mask_band*n_mask + n_gender, \n",
    "                n_age_band*n_age + n_mask_band*n_mask + n_gender,\n",
    "                64,\n",
    "                1,\n",
    "                0\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.total_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_disc+n_age_band*n_age + n_mask_band*n_mask + n_gender, n_disc*2,4,2,1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(n_disc*2,n_disc*4,4,2,1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(n_disc*4,n_disc*8,4,2,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_common = nn.Sequential(\n",
    "            nn.Linear(8*8*img_size,1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_head1 = nn.Sequential(\n",
    "            nn.Linear(1024,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc_head2 = nn.Sequential(\n",
    "            nn.Linear(1024,n_age_band),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        self.fc_head3 = nn.Sequential(\n",
    "            nn.Linear(1024,n_mask_band),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self,img,age,mask,gender):\n",
    "        l = age.repeat(1,n_age,1,1,)\n",
    "        m= mask.repeat(1,n_mask,1,1,)\n",
    "        k = gender.repeat(1,n_gender,1,1,)\n",
    "        conv_img = self.conv_img(img)\n",
    "        conv_l   = self.conv_l(torch.cat([l,m,k],dim=1))\n",
    "        catted   = torch.cat((conv_img,conv_l),dim=1)\n",
    "        total_conv = self.total_conv(catted).view(-1,8*8*img_size)\n",
    "        body = self.fc_common(total_conv)\n",
    "        \n",
    "        head1 = self.fc_head1(body)\n",
    "        head2 = self.fc_head2(body)\n",
    "        head3 = self.fc_head3(body)\n",
    "        \n",
    "        return head1,head2,head3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "254f80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dz(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dz,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_z,n_disc*4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(n_disc*4,n_disc*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(n_disc*2,n_disc),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(n_disc,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1af59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    netE = Encoder().cuda()\n",
    "    netD_img = Dimg().cuda()\n",
    "    netD_z  = Dz().cuda()\n",
    "    netG = Generator().cuda()\n",
    "else:\n",
    "    netE = Encoder()\n",
    "    netD_img = Dimg()\n",
    "    netD_z  = Dz()\n",
    "    netG = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82af5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find(\"Linear\") !=-1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0f8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "netE.apply(weights_init)\n",
    "netD_img.apply(weights_init)\n",
    "netD_z.apply(weights_init)\n",
    "netG.apply(weights_init)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "567e16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerE = optim.Adam(netE.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "optimizerD_z = optim.Adam(netD_z.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "optimizerD_img = optim.Adam(netD_img.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(),lr=0.0002,betas=(0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbe5e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labelTensor, batchSize, n_l):\n",
    "    oneHot = - torch.ones(batchSize*n_l).view(batchSize,n_l)\n",
    "    for i,j in enumerate(labelTensor):\n",
    "        oneHot[i,j] = 1\n",
    "    if use_cuda:\n",
    "        return Variable(oneHot).cuda()\n",
    "    else:\n",
    "        return Variable(oneHot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b1a5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    BCE = nn.BCELoss().cuda()\n",
    "    L1  = nn.L1Loss().cuda()\n",
    "    CE = nn.CrossEntropyLoss().cuda()\n",
    "    MSE = nn.MSELoss().cuda()\n",
    "else:\n",
    "    BCE = nn.BCELoss()\n",
    "    L1  = nn.L1Loss()\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "    MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c395a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image differentiation loss\n",
    "def TV_LOSS(imgTensor,img_size=128):\n",
    "    x = (imgTensor[:,:,1:,:]-imgTensor[:,:,:img_size-1,:])**2\n",
    "    y = (imgTensor[:,:,:,1:]-imgTensor[:,:,:,:img_size-1])**2\n",
    "\n",
    "    out = (x.mean(dim=2)+y.mean(dim=3)).mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e338fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "niter=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44f26d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "outf='./result_tv_gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08b435a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {\"female\" : 0, \"male\" : 1}\n",
    "age_dict = {\"[0,30)\" : 0,\"[30,60)\" : 1,\"[60,inf)\" : 2}\n",
    "mask_dict = {\"mask\":0, \"normal\":1, \"incorrect\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7bb610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:1, step:220\n",
      "EG_L1_loss:0.129450           |G_img_loss:2.120445           \n",
      "G_tv_loss:0.000393            |Ez_loss:0.909078\n",
      "D_img:0.741352                |D_reconst:0.125954            |D_loss:0.460743               \n",
      "D_z:0.414463                  |D_z_prior:0.580946            |Dz_loss:1.093193              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:2, step:220\n",
      "EG_L1_loss:0.131501           |G_img_loss:2.780546           \n",
      "G_tv_loss:0.000449            |Ez_loss:0.600468\n",
      "D_img:0.893039                |D_reconst:0.104474            |D_loss:0.258619               \n",
      "D_z:0.550022                  |D_z_prior:0.526350            |Dz_loss:1.446713              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:3, step:220\n",
      "EG_L1_loss:0.118738           |G_img_loss:2.554826           \n",
      "G_tv_loss:0.000369            |Ez_loss:0.618834\n",
      "D_img:0.891201                |D_reconst:0.091069            |D_loss:0.233443               \n",
      "D_z:0.539041                  |D_z_prior:0.552935            |Dz_loss:1.368851              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:4, step:220\n",
      "EG_L1_loss:0.108173           |G_img_loss:3.119889           \n",
      "G_tv_loss:0.000464            |Ez_loss:0.872858\n",
      "D_img:0.596208                |D_reconst:0.138103            |D_loss:1.031434               \n",
      "D_z:0.424995                  |D_z_prior:0.617867            |Dz_loss:1.044407              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:5, step:220\n",
      "EG_L1_loss:0.104643           |G_img_loss:3.476090           \n",
      "G_tv_loss:0.000443            |Ez_loss:1.186899\n",
      "D_img:0.957232                |D_reconst:0.044786            |D_loss:0.096799               \n",
      "D_z:0.318179                  |D_z_prior:0.745180            |Dz_loss:0.692297              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:6, step:220\n",
      "EG_L1_loss:0.112329           |G_img_loss:2.649827           \n",
      "G_tv_loss:0.000395            |Ez_loss:1.031396\n",
      "D_img:0.847013                |D_reconst:0.192663            |D_loss:0.556988               \n",
      "D_z:0.413521                  |D_z_prior:0.627748            |Dz_loss:1.055190              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:7, step:220\n",
      "EG_L1_loss:0.104914           |G_img_loss:5.023325           \n",
      "G_tv_loss:0.000355            |Ez_loss:0.544446\n",
      "D_img:0.866104                |D_reconst:0.016757            |D_loss:0.214052               \n",
      "D_z:0.589246                  |D_z_prior:0.528115            |Dz_loss:1.568486              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:8, step:220\n",
      "EG_L1_loss:0.097101           |G_img_loss:3.706873           \n",
      "G_tv_loss:0.000475            |Ez_loss:0.611582\n",
      "D_img:0.889366                |D_reconst:0.036888            |D_loss:0.289319               \n",
      "D_z:0.547099                  |D_z_prior:0.540701            |Dz_loss:1.422571              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:9, step:220\n",
      "EG_L1_loss:0.100529           |G_img_loss:3.207977           \n",
      "G_tv_loss:0.000464            |Ez_loss:0.759900\n",
      "D_img:0.997806                |D_reconst:0.066959            |D_loss:0.073449               \n",
      "D_z:0.492823                  |D_z_prior:0.519816            |Dz_loss:1.386104              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:10, step:220\n",
      "EG_L1_loss:0.092423           |G_img_loss:3.912376           \n",
      "G_tv_loss:0.000508            |Ez_loss:0.703780\n",
      "D_img:0.881885                |D_reconst:0.038130            |D_loss:0.219603               \n",
      "D_z:0.517099                  |D_z_prior:0.502524            |Dz_loss:1.460001              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:11, step:220\n",
      "EG_L1_loss:0.103895           |G_img_loss:4.045201           \n",
      "G_tv_loss:0.000429            |Ez_loss:0.825168\n",
      "D_img:0.833298                |D_reconst:0.094150            |D_loss:0.593424               \n",
      "D_z:0.466853                  |D_z_prior:0.530810            |Dz_loss:1.304472              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:12, step:220\n",
      "EG_L1_loss:0.101959           |G_img_loss:2.704083           \n",
      "G_tv_loss:0.000694            |Ez_loss:0.916626\n",
      "D_img:0.859293                |D_reconst:0.091489            |D_loss:0.290564               \n",
      "D_z:0.407340                  |D_z_prior:0.503111            |Dz_loss:1.236123              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:13, step:220\n",
      "EG_L1_loss:0.094683           |G_img_loss:3.538150           \n",
      "G_tv_loss:0.000712            |Ez_loss:0.810950\n",
      "D_img:0.885508                |D_reconst:0.062624            |D_loss:0.317064               \n",
      "D_z:0.455140                  |D_z_prior:0.575481            |Dz_loss:1.182415              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:14, step:220\n",
      "EG_L1_loss:0.097194           |G_img_loss:4.760029           \n",
      "G_tv_loss:0.000683            |Ez_loss:0.877413\n",
      "D_img:0.979721                |D_reconst:0.055543            |D_loss:0.083673               \n",
      "D_z:0.431149                  |D_z_prior:0.517714            |Dz_loss:1.242239              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:15, step:220\n",
      "EG_L1_loss:0.100675           |G_img_loss:5.366901           \n",
      "G_tv_loss:0.000497            |Ez_loss:0.817644\n",
      "D_img:0.964751                |D_reconst:0.015807            |D_loss:0.053612               \n",
      "D_z:0.443090                  |D_z_prior:0.529132            |Dz_loss:1.245249              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:16, step:220\n",
      "EG_L1_loss:0.088604           |G_img_loss:5.661475           \n",
      "G_tv_loss:0.000483            |Ez_loss:0.656614\n",
      "D_img:0.977241                |D_reconst:0.006795            |D_loss:0.031975               \n",
      "D_z:0.534208                  |D_z_prior:0.580799            |Dz_loss:1.334365              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:17, step:220\n",
      "EG_L1_loss:0.091363           |G_img_loss:3.462255           \n",
      "G_tv_loss:0.000595            |Ez_loss:0.805984\n",
      "D_img:0.864305                |D_reconst:0.088524            |D_loss:0.311519               \n",
      "D_z:0.458825                  |D_z_prior:0.537130            |Dz_loss:1.254127              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:18, step:220\n",
      "EG_L1_loss:0.079634           |G_img_loss:5.668915           \n",
      "G_tv_loss:0.000717            |Ez_loss:0.737957\n",
      "D_img:0.831779                |D_reconst:0.076967            |D_loss:0.383206               \n",
      "D_z:0.494600                  |D_z_prior:0.514261            |Dz_loss:1.395814              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:19, step:220\n",
      "EG_L1_loss:0.092605           |G_img_loss:3.739531           \n",
      "G_tv_loss:0.000765            |Ez_loss:0.754275\n",
      "D_img:0.882391                |D_reconst:0.127250            |D_loss:0.308780               \n",
      "D_z:0.486563                  |D_z_prior:0.496767            |Dz_loss:1.391578              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:20, step:220\n",
      "EG_L1_loss:0.076451           |G_img_loss:2.930142           \n",
      "G_tv_loss:0.000554            |Ez_loss:0.842660\n",
      "D_img:0.962447                |D_reconst:0.162713            |D_loss:0.270115               \n",
      "D_z:0.438226                  |D_z_prior:0.470919            |Dz_loss:1.365765              \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "epoch:21, step:220\n",
      "EG_L1_loss:0.077268           |G_img_loss:4.246662           \n",
      "G_tv_loss:0.000723            |Ez_loss:1.008365\n",
      "D_img:0.940900                |D_reconst:0.095094            |D_loss:0.186428               \n",
      "D_z:0.380896                  |D_z_prior:0.597964            |Dz_loss:1.014157              \n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:22, step:220\n",
      "EG_L1_loss:0.068589           |G_img_loss:1.925667           \n",
      "G_tv_loss:0.000901            |Ez_loss:1.115818\n",
      "D_img:0.736998                |D_reconst:0.224848            |D_loss:0.693766               \n",
      "D_z:0.357458                  |D_z_prior:0.536012            |Dz_loss:1.109993              \n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(niter):\n",
    "    for i,(img_data, img_label) in enumerate(dataloader):\n",
    "        # make image variable and class variable\n",
    "        \n",
    "        img_data_v = Variable(img_data)\n",
    "        label = list(map(lambda x : dataset.classes[x], img_label))\n",
    "        img_gender = list(map(lambda x : gender_dict[x.split(\"_\")[0]], label))\n",
    "        img_age = list(map(lambda x : age_dict[x.split(\"_\")[1]], label))\n",
    "        img_mask = list(map(lambda x : mask_dict[x.split(\"_\")[2]], label))\n",
    "        \n",
    "        img_age = torch.tensor(img_age)\n",
    "        img_gender = torch.tensor(img_gender)\n",
    "        \n",
    "        img_age_v = Variable(img_age).view(-1,1)\n",
    "        img_gender_v = Variable(img_gender.float())\n",
    "        \n",
    "        if epoch == 0 and i == 0:\n",
    "            print(\"first_step\")\n",
    "            \n",
    "            num_image = 8\n",
    "            num_age = 3\n",
    "            num_mask = 3\n",
    "\n",
    "            fixed_noise = img_data[:num_image].repeat(num_age*num_mask,1,1,1)\n",
    "            fixed_age = -1 * torch.ones((num_image*num_age*num_mask,num_age))\n",
    "            for i,l in enumerate(fixed_age):\n",
    "                l[i//(num_image * num_age)] = 1\n",
    "            fixed_mask = -1 * torch.ones((num_image*num_age*num_mask,num_age))\n",
    "            for i,l in enumerate(fixed_mask):\n",
    "                l[(i//num_image)% num_age] = 1\n",
    "            fixed_g = img_gender[:num_image].view(-1,1).repeat(num_age*num_mask,1)\n",
    "            \n",
    "            fixed_img_v = Variable(fixed_noise)\n",
    "            fixed_g_v = Variable(fixed_g)\n",
    "            fixed_age_v = Variable(fixed_age)\n",
    "            fixed_mask_v = Variable(fixed_mask)\n",
    "\n",
    "            pickle.dump(fixed_noise,open(\"fixed_noise.p\",\"wb\"))\n",
    "\n",
    "            if use_cuda:\n",
    "                fixed_img_v = fixed_img_v.cuda()\n",
    "                fixed_g_v = fixed_g_v.cuda()\n",
    "                fixed_age_v = fixed_age_v.cuda()\n",
    "                fixed_mask_v = fixed_mask_v.cuda()\n",
    "        \n",
    "        if use_cuda:\n",
    "            img_data_v = img_data_v.cuda()\n",
    "            img_age_v = img_age_v.cuda()\n",
    "            img_gender_v = img_gender_v.cuda()\n",
    "        \n",
    "        # make one hot encoding version of label\n",
    "        batchSize = img_data_v.size(0)\n",
    "        age_ohe = one_hot(img_age,batchSize,3)\n",
    "        mask_ohe = one_hot(img_age,batchSize,3)\n",
    "        \n",
    "        # prior distribution z_star, real_label, fake_label\n",
    "        z_star = Variable(torch.FloatTensor(batchSize*n_z).uniform_(-1,1)).view(batchSize,n_z)\n",
    "        real_label = Variable(torch.ones(batchSize).fill_(1)).view(-1,1)\n",
    "        fake_label = Variable(torch.ones(batchSize).fill_(0)).view(-1,1)\n",
    "        if use_cuda:\n",
    "            z_star, real_label, fake_label = z_star.cuda(),real_label.cuda(),fake_label.cuda()\n",
    "        \n",
    "        ## train Encoder and Generator with reconstruction loss\n",
    "        netE.zero_grad()\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # EG_loss 1. L1 reconstruction loss\n",
    "        z = netE(img_data_v)\n",
    "        reconst = netG(z,age_ohe,img_gender_v,mask_ohe)\n",
    "        EG_L1_loss = L1(reconst,img_data_v)\n",
    "        \n",
    "        # EG_loss 2. GAN loss - image\n",
    "        z = netE(img_data_v)\n",
    "        reconst = netG(z,age_ohe,img_gender_v,mask_ohe)\n",
    "        D_reconst,_,_ = netD_img(\n",
    "            reconst,\n",
    "            age_ohe.view(batchSize,n_age_band,1,1),\n",
    "            mask_ohe.view(batchSize,n_mask_band,1,1),\n",
    "            img_gender_v.view(batchSize,1,1,1),\n",
    "        )\n",
    "        G_img_loss = BCE(D_reconst,real_label)\n",
    "                \n",
    "        ## EG_loss 3. GAN loss - z \n",
    "        Dz_prior = netD_z(z_star)\n",
    "        Dz = netD_z(z)\n",
    "        Ez_loss = BCE(Dz,real_label)\n",
    "        \n",
    "        ## EG_loss 4. TV loss - G\n",
    "        reconst = netG(z.detach(),age_ohe,img_gender_v,mask_ohe)\n",
    "        G_tv_loss = TV_LOSS(reconst)\n",
    "        \n",
    "        EG_loss = EG_L1_loss + 0.0001*G_img_loss + 0.01*Ez_loss + G_tv_loss\n",
    "        EG_loss.backward()\n",
    "        \n",
    "        optimizerE.step()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        ## train netD_z with prior distribution U(-1,1)\n",
    "        netD_z.zero_grad()        \n",
    "        Dz_prior = netD_z(z_star)\n",
    "        Dz = netD_z(z.detach())\n",
    "        \n",
    "        Dz_loss = BCE(Dz_prior,real_label)+BCE(Dz,fake_label)\n",
    "        Dz_loss.backward()\n",
    "        optimizerD_z.step()\n",
    "        \n",
    "        ## train D_img with real images\n",
    "        netD_img.zero_grad()\n",
    "        D_img,D_age,D_mask = netD_img(\n",
    "            img_data_v,\n",
    "            age_ohe.view(batchSize,n_age_band,1,1),\n",
    "            mask_ohe.view(batchSize,n_mask_band,1,1),\n",
    "            img_gender_v.view(batchSize,1,1,1),\n",
    "        )\n",
    "        D_reconst,_,_ = netD_img(\n",
    "            reconst.detach(),\n",
    "            age_ohe.view(batchSize,n_age_band,1,1),\n",
    "            mask_ohe.view(batchSize,n_mask_band,1,1),\n",
    "            img_gender_v.view(batchSize,1,1,1),\n",
    "        )\n",
    "\n",
    "        D_loss = BCE(D_img,real_label)+BCE(D_reconst,fake_label)\n",
    "        D_loss.backward()\n",
    "        optimizerD_img.step()\n",
    "        \n",
    "        #break\n",
    "        #print(EG_L1_loss)\n",
    "    \n",
    "    ## save fixed img for every 20 step        \n",
    "    fixed_z = netE(fixed_img_v)\n",
    "    fixed_fake = netG(fixed_z,fixed_age_v,fixed_g_v,fixed_mask_v)\n",
    "    vutils.save_image(fixed_fake.data,\n",
    "                '%s/reconst_epoch%03d.png' % (outf,epoch+1),\n",
    "                normalize=True)\n",
    "    \n",
    "    ## checkpoint\n",
    "    if epoch%10==0:\n",
    "        torch.save(netE.state_dict(),\"%s/netE_%03d.pth\"%(outf,epoch+1))\n",
    "        torch.save(netG.state_dict(),\"%s/netG_%03d.pth\"%(outf,epoch+1))\n",
    "        torch.save(netD_img.state_dict(),\"%s/netD_img_%03d.pth\"%(outf,epoch+1))\n",
    "        torch.save(netD_z.state_dict(),\"%s/netD_z_%03d.pth\"%(outf,epoch+1))\n",
    "    \n",
    "    msg1 = \"epoch:{}, step:{}\".format(epoch+1,i+1)\n",
    "    msg2 = format(\"EG_L1_loss:%f\"%(EG_L1_loss),\"<30\")+\"|\"+format(\"G_img_loss:%f\"%(G_img_loss),\"<30\")\n",
    "    msg5 = format(\"G_tv_loss:%f\"%(G_tv_loss),\"<30\")+\"|\"+\"Ez_loss:%f\"%(Ez_loss)\n",
    "    msg3 = format(\"D_img:%f\"%(D_img.mean()),\"<30\")+\"|\"+format(\"D_reconst:%f\"%(D_reconst.mean()),\"<30\")\\\n",
    "    +\"|\"+format(\"D_loss:%f\"%(D_loss),\"<30\")\n",
    "    msg4 = format(\"D_z:%f\"%(Dz.mean()),\"<30\")+\"|\"+format(\"D_z_prior:%f\"%(Dz_prior.mean()),\"<30\")\\\n",
    "    +\"|\"+format(\"Dz_loss:%f\"%(Dz_loss),\"<30\")\n",
    "\n",
    "    print()\n",
    "    print(msg1)\n",
    "    print(msg2)\n",
    "    print(msg5)\n",
    "    print(msg3)\n",
    "    print(msg4)       \n",
    "    print()\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e470ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image = 5\n",
    "num_age = 3\n",
    "num_mask = 3\n",
    "\n",
    "fixed_age = -1 * torch.ones((num_image*num_age*num_mask,3))\n",
    "for i,l in enumerate(fixed_age):\n",
    "    l[i//(num_image * num_age)] = 1\n",
    "fixed_age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image = 5\n",
    "num_age = 3\n",
    "num_mask = 3\n",
    "\n",
    "fixed_mask = -1 * torch.ones((num_image*num_age*num_mask,3))\n",
    "for i,l in enumerate(fixed_mask):\n",
    "    l[(i//num_image)% num_age] = 1\n",
    "fixed_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8fbe27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
