{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa234a5-a5ea-435a-9d0b-736646930b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run just once\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f549dc6-701a-4dcb-b600-e3b94f2a5748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, ConcatDataset\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import F1\n",
    "\n",
    "from Dataset import AgeDataset\n",
    "from myutil import mapAttributes, save_aug, mapAge\n",
    "from model import MyResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56cf517a-6758-40e9-86cd-81105554844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05030b4c-f930-4f69-be7d-1f3a93af53b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:24<00:00,  7.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# # run just once! (takes about 30 sec.)\n",
    "# base_path = \"../input/data/train\"\n",
    "# df = pd.read_csv(os.path.join(base_path, \"train.csv\"))\n",
    "# old_labels = df.loc[df.age>=60]\n",
    "# myaug = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ColorJitter(contrast=0.1),\n",
    "#     transforms.RandomPosterize(bits=2),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# save_aug(myaug, old_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0049a9ba-2e7e-47a7-a735-51410a365015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed (이렇게 하면 되는건가.. 흠)\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "     \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8406b-7e61-4129-8f01-11d25235356a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## split train, val folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974fd439-cff4-4035-9c2e-394349e103f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 579)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = \"/opt/ml/input/data/train\"\n",
    "\n",
    "# combine two dataframes (original + augmented)\n",
    "df1 = pd.read_csv(os.path.join(base_path, \"train.csv\"))\n",
    "df2 = pd.read_csv(os.path.join(base_path, \"old_path_augmented.csv\"))\n",
    "df = df1.append(df2)  # 2892 rows (= 2700 + 192)\n",
    "\n",
    "y_data = df.apply(lambda x: mapAge(x['path']), axis=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(df.index, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "len(x_train), len(x_val)  # 2313, 579 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9564b7a-f283-46c1-bace-d2ee0b718ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ff522-f027-429f-b3f9-94204453e0ef",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7551361c-6d5b-44c0-95e3-789458df9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2313/2313 [00:00<00:00, 19241.15it/s]\n",
      "100%|██████████| 579/579 [00:00<00:00, 20161.74it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(base_path, \"train.csv\"))\n",
    "\n",
    "train_dataset = AgeDataset(\n",
    "    data_path = base_path,\n",
    "    data = df.loc[x_train],\n",
    "    transform = transforms.Compose([\n",
    "         transforms.Resize((128, 128)),\n",
    "         transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_dataset = AgeDataset(\n",
    "    data_path = base_path,\n",
    "    data = df.loc[x_val],\n",
    "    transform = transforms.Compose([\n",
    "         transforms.Resize((128, 128)),\n",
    "         transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae3e89b-213c-47af-8bb7-7d0448571830",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=4,\n",
    "    drop_last = True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=4,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "# x, y = next(iter(train_loader))\n",
    "# x.shape, y.shape  # (torch.Size([32, 3, 128, 128]), torch.Size([32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97732ae7-b97a-467c-890b-f022712d9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss, optm\n",
    "resnet18 = MyResNet18(num_classes=3).to(DEVICE)\n",
    "multi_criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "optm = torch.optim.Adam(resnet18.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d47e790d-c005-45f7-ae9e-0e5e8378f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates accuracy\n",
    "def get_acc(y_pred, y_test):  # torch.Size([32, 3]) torch.Size([32])\n",
    "    output = torch.argmax(y_pred, dim=1)\n",
    "    correct = sum(output == y_test)/len(output)\n",
    "    return torch.sum(correct)\n",
    "\n",
    "# labels with proper dimension\n",
    "def labeling(x, num_labels):\n",
    "    output = torch.zeros((x.shape[0], num_labels))\n",
    "    for i in range(x.shape[0]):\n",
    "        output[i,x[i]] = 1\n",
    "    return output\n",
    "\n",
    "# F1 score\n",
    "f1 = F1(num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3aabff-57c1-4a61-bc06-b23e39c977ac",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98460be9-a5d9-49ca-a97f-4a645d703a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:24<11:42, 24.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.3161337375640869 train_accuracy_avg: 0.810519814491272 train_f1_avg: tensor(0.8105)\n",
      "val_loss_avg: 0.24205219745635986 val_accuracy_avg: 0.85317462682724 val_f1_avg: tensor(0.8532)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:48<11:17, 24.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.17072679102420807 train_accuracy_avg: 0.915099024772644 train_f1_avg: tensor(0.9151)\n",
      "val_loss_avg: 0.22901587188243866 val_accuracy_avg: 0.8764881491661072 val_f1_avg: tensor(0.8765)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:12<10:52, 24.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.10740253329277039 train_accuracy_avg: 0.9539604187011719 train_f1_avg: tensor(0.9540)\n",
      "val_loss_avg: 0.23859208822250366 val_accuracy_avg: 0.8841766119003296 val_f1_avg: tensor(0.8842)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:36<10:27, 24.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.06800457090139389 train_accuracy_avg: 0.9760519862174988 train_f1_avg: tensor(0.9761)\n",
      "val_loss_avg: 0.20037534832954407 val_accuracy_avg: 0.905754029750824 val_f1_avg: tensor(0.9058)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [02:00<10:05, 24.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.052631959319114685 train_accuracy_avg: 0.9801980257034302 train_f1_avg: tensor(0.9802)\n",
      "val_loss_avg: 0.20469212532043457 val_accuracy_avg: 0.906994104385376 val_f1_avg: tensor(0.9070)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:25<09:40, 24.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.03425491601228714 train_accuracy_avg: 0.9879332184791565 train_f1_avg: tensor(0.9879)\n",
      "val_loss_avg: 0.19832223653793335 val_accuracy_avg: 0.9126984477043152 val_f1_avg: tensor(0.9127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [02:49<09:17, 24.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.020756512880325317 train_accuracy_avg: 0.9931930899620056 train_f1_avg: tensor(0.9932)\n",
      "val_loss_avg: 0.22970439493656158 val_accuracy_avg: 0.9060020446777344 val_f1_avg: tensor(0.9060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [03:14<08:56, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.015586357563734055 train_accuracy_avg: 0.9954826831817627 train_f1_avg: tensor(0.9955)\n",
      "val_loss_avg: 0.24454696476459503 val_accuracy_avg: 0.9084821939468384 val_f1_avg: tensor(0.9085)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [03:39<08:37, 24.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.012813189998269081 train_accuracy_avg: 0.997029721736908 train_f1_avg: tensor(0.9970)\n",
      "val_loss_avg: 0.2691914737224579 val_accuracy_avg: 0.90327388048172 val_f1_avg: tensor(0.9033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [04:03<08:12, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.02396000176668167 train_accuracy_avg: 0.9909653663635254 train_f1_avg: tensor(0.9910)\n",
      "val_loss_avg: 0.31640681624412537 val_accuracy_avg: 0.8829365372657776 val_f1_avg: tensor(0.8829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [04:28<07:48, 24.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.024752095341682434 train_accuracy_avg: 0.9878712892532349 train_f1_avg: tensor(0.9879)\n",
      "val_loss_avg: 0.27101826667785645 val_accuracy_avg: 0.8985615372657776 val_f1_avg: tensor(0.8986)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [04:53<07:24, 24.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.013903932645916939 train_accuracy_avg: 0.9946163892745972 train_f1_avg: tensor(0.9946)\n",
      "val_loss_avg: 0.2775839865207672 val_accuracy_avg: 0.8953373432159424 val_f1_avg: tensor(0.8953)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [05:17<06:57, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.004060744773596525 train_accuracy_avg: 0.9988861680030823 train_f1_avg: tensor(0.9989)\n",
      "val_loss_avg: 0.2764400541782379 val_accuracy_avg: 0.90327388048172 val_f1_avg: tensor(0.9033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [05:41<06:31, 24.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.0016956357285380363 train_accuracy_avg: 0.9999381303787231 train_f1_avg: tensor(0.9999)\n",
      "val_loss_avg: 0.285016268491745 val_accuracy_avg: 0.9005457162857056 val_f1_avg: tensor(0.9005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [06:06<06:06, 24.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.001063520205207169 train_accuracy_avg: 0.9999381303787231 train_f1_avg: tensor(0.9999)\n",
      "val_loss_avg: 0.30449268221855164 val_accuracy_avg: 0.9015377759933472 val_f1_avg: tensor(0.9015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [06:30<05:40, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.0010539666982367635 train_accuracy_avg: 0.9999381303787231 train_f1_avg: tensor(0.9999)\n",
      "val_loss_avg: 0.31167715787887573 val_accuracy_avg: 0.9027778506278992 val_f1_avg: tensor(0.9028)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [06:54<05:16, 24.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.028254186734557152 train_accuracy_avg: 0.9873144030570984 train_f1_avg: tensor(0.9873)\n",
      "val_loss_avg: 0.26927241683006287 val_accuracy_avg: 0.9012897610664368 val_f1_avg: tensor(0.9013)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [07:18<04:51, 24.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.034952856600284576 train_accuracy_avg: 0.9835396409034729 train_f1_avg: tensor(0.9835)\n",
      "val_loss_avg: 0.32314732670783997 val_accuracy_avg: 0.8918651342391968 val_f1_avg: tensor(0.8919)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [07:43<04:27, 24.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.005068671423941851 train_accuracy_avg: 0.9983910918235779 train_f1_avg: tensor(0.9984)\n",
      "val_loss_avg: 0.30836305022239685 val_accuracy_avg: 0.9005457162857056 val_f1_avg: tensor(0.9005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [08:07<04:03, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.0020776838064193726 train_accuracy_avg: 0.9994431138038635 train_f1_avg: tensor(0.9994)\n",
      "val_loss_avg: 0.3095111548900604 val_accuracy_avg: 0.9035218954086304 val_f1_avg: tensor(0.9035)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [08:32<03:39, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.0016063719522207975 train_accuracy_avg: 0.9996906518936157 train_f1_avg: tensor(0.9997)\n",
      "val_loss_avg: 0.2963673770427704 val_accuracy_avg: 0.9102182984352112 val_f1_avg: tensor(0.9102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [08:56<03:14, 24.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.0005533727817237377 train_accuracy_avg: 0.9999381303787231 train_f1_avg: tensor(0.9999)\n",
      "val_loss_avg: 0.3006106913089752 val_accuracy_avg: 0.9097222685813904 val_f1_avg: tensor(0.9097)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [09:20<02:50, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.00035639957059174776 train_accuracy_avg: 1.0 train_f1_avg: tensor(1.)\n",
      "val_loss_avg: 0.30650487542152405 val_accuracy_avg: 0.9114583730697632 val_f1_avg: tensor(0.9115)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [09:45<02:25, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.00026701774913817644 train_accuracy_avg: 1.0 train_f1_avg: tensor(1.)\n",
      "val_loss_avg: 0.31427621841430664 val_accuracy_avg: 0.9112103581428528 val_f1_avg: tensor(0.9112)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [10:09<02:01, 24.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.000493402243591845 train_accuracy_avg: 0.9999381303787231 train_f1_avg: tensor(0.9999)\n",
      "val_loss_avg: 0.3322078585624695 val_accuracy_avg: 0.9112103581428528 val_f1_avg: tensor(0.9112)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [10:33<01:37, 24.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.021138319745659828 train_accuracy_avg: 0.9901609420776367 train_f1_avg: tensor(0.9902)\n",
      "val_loss_avg: 0.42308419942855835 val_accuracy_avg: 0.8415179252624512 val_f1_avg: tensor(0.8415)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [10:57<01:12, 24.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.05035959929227829 train_accuracy_avg: 0.973948061466217 train_f1_avg: tensor(0.9739)\n",
      "val_loss_avg: 0.3325302004814148 val_accuracy_avg: 0.8923611640930176 val_f1_avg: tensor(0.8924)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [11:22<00:48, 24.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.007511787582188845 train_accuracy_avg: 0.9969678521156311 train_f1_avg: tensor(0.9970)\n",
      "val_loss_avg: 0.39886611700057983 val_accuracy_avg: 0.8869048357009888 val_f1_avg: tensor(0.8869)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [11:46<00:24, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.007497994229197502 train_accuracy_avg: 0.9961014986038208 train_f1_avg: tensor(0.9961)\n",
      "val_loss_avg: 0.34848788380622864 val_accuracy_avg: 0.8990575671195984 val_f1_avg: tensor(0.8991)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [12:10<00:00, 24.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg: 0.002167279366403818 train_accuracy_avg: 0.9994431138038635 train_f1_avg: tensor(0.9994)\n",
      "val_loss_avg: 0.3560841977596283 val_accuracy_avg: 0.8960813879966736 val_f1_avg: tensor(0.8961)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "tr_writer = SummaryWriter(\"logs/exp_12/tr\")\n",
    "val_writer = SummaryWriter(\"logs/exp_12/val\")\n",
    "\n",
    "for ep in tqdm(range(EPOCHS)):\n",
    "\n",
    "    # == training phase =================================\n",
    "    train_loss_ep = 0\n",
    "    train_accuracy_ep = 0\n",
    "    train_f1_ep = 0\n",
    "    for X, y in iter(train_loader):\n",
    "        \n",
    "        resnet18.train()\n",
    "        \n",
    "        # change dim for loss func\n",
    "        _y = labeling(y.unsqueeze(1), 3)  # torch.Size([32, 3])\n",
    "        X, _y = X.to(DEVICE), _y.to(DEVICE)\n",
    "        \n",
    "        predict = resnet18(X)  # torch.Size([32, 3])\n",
    "        \n",
    "        train_loss = multi_criterion(predict, _y)\n",
    "        train_accuracy = get_acc(predict, y.squeeze().to(DEVICE))\n",
    "        train_f1 = f1(torch.argmax(predict, 1).to(\"cpu\"), y)\n",
    "        \n",
    "        optm.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "        optm.step()\n",
    "        \n",
    "        train_loss_ep += train_loss\n",
    "        train_accuracy_ep += train_accuracy\n",
    "        train_f1_ep += train_f1\n",
    "        # end of one dataloader\n",
    "    \n",
    "    train_loss_avg = train_loss_ep / len(train_loader)\n",
    "    train_accuracy_avg = train_accuracy_ep / len(train_loader)\n",
    "    train_f1_avg = train_f1_ep / len(train_loader)\n",
    "    \n",
    "    # tensorboard\n",
    "    tr_writer.add_scalar(\n",
    "        'train_loss_avg',\n",
    "        train_loss_avg,\n",
    "        ep\n",
    "    )\n",
    "    tr_writer.add_scalar(\n",
    "        'train_accuracy_avg',\n",
    "        train_accuracy_avg,\n",
    "        ep\n",
    "    )\n",
    "    tr_writer.add_scalar(\n",
    "        'train_f1_avg',\n",
    "        train_f1_avg,\n",
    "        ep\n",
    "    )\n",
    "    \n",
    "    # === testing phase =================================\n",
    "    with torch.no_grad():\n",
    "        val_loss_ep = 0\n",
    "        val_accuracy_ep = 0\n",
    "        val_f1_ep = 0\n",
    "        for X, y in iter(val_loader):\n",
    "            ## testing phrase\n",
    "            resnet18.eval()\n",
    "\n",
    "            # change dim for loss func\n",
    "            _y = labeling(y.unsqueeze(1), 3)\n",
    "\n",
    "            X, _y = X.to(DEVICE), _y.to(DEVICE)\n",
    "            predict = resnet18(X)\n",
    "\n",
    "            val_loss = multi_criterion(predict, _y)\n",
    "            val_accuracy = get_acc(predict, y.squeeze().to(DEVICE))\n",
    "            val_f1 = f1(torch.argmax(predict, 1).to(\"cpu\"), y)\n",
    "\n",
    "            val_loss_ep += val_loss\n",
    "            val_accuracy_ep += val_accuracy\n",
    "            val_f1_ep += val_f1\n",
    "\n",
    "\n",
    "        val_loss_avg = val_loss_ep / len(val_loader)\n",
    "        val_accuracy_avg = val_accuracy_ep / len(val_loader)\n",
    "        val_f1_avg = val_f1_ep / len(val_loader)\n",
    "    \n",
    "    # tensorboard\n",
    "    val_writer.add_scalar(\n",
    "        'val_loss_avg',\n",
    "        val_loss_avg,\n",
    "        ep\n",
    "    )\n",
    "    val_writer.add_scalar(\n",
    "        'val_accuracy_avg',\n",
    "        val_accuracy_avg,\n",
    "        ep\n",
    "    )\n",
    "    val_writer.add_scalar(\n",
    "        'val_f1_avg',\n",
    "        val_f1_avg,\n",
    "        ep\n",
    "    )\n",
    "    \n",
    "    print(\"train_loss_avg:\", train_loss_avg.item(), \"train_accuracy_avg:\", train_accuracy_avg.item(), \"train_f1_avg:\", train_f1_avg)\n",
    "    print(\"val_loss_avg:\", val_loss_avg.item(), \"val_accuracy_avg:\", val_accuracy_avg.item(), \"val_f1_avg:\", val_f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1455bddf-a9d5-410a-a1d8-fc8eced45cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How to Save\n",
    "\n",
    "# best_val_loss = 0\n",
    "# best_val_f1 = 0\n",
    "\n",
    "# for ep in tqdm(range(EPOCHS)):\n",
    "    \n",
    "#     # train, valid ...\n",
    "    \n",
    "#     if best_val_f1 < val_f1_avg or best_val_loss >= val_loss_avg:\n",
    "#         best_val_loss = val_loss_avg\n",
    "#         best_val_f1 = val_f1_avg\n",
    "#         torch.save(resnet18.state_dict(), f\"{resnet18.__class__.__name__}_{ep}_loss{train_loss_avg:.2f}_acc{train_accuracy_avg:2f}.pt\")\n",
    "        \n",
    "# #         torch.save({\n",
    "# #             'epoch': ep,\n",
    "# #             'model_state_dict': resnet18.state_dict(),\n",
    "# #             'optimizer_state_dict': optm.state_dict(),\n",
    "# #             'loss': train_loss_avg,\n",
    "# #             }, f\"saved/{resnet18.__class__.__name__}_{ep}_loss{train_loss_avg:.2f}_acc{train_accuracy_avg:2f}.pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
